# ================================================================================
# DDoS-Detector — Unified Configuration File
# ================================================================================
# Central configuration for ALL project scripts:
#   - genetic_algorithm.py  (Genetic Algorithm Feature Selection)
#   - stacking.py           (Stacking Ensemble Classifiers with Multi-class Support)
#   - wgangp.py             (WGAN-GP Data Augmentation)
#
# Configuration priority (highest → lowest):
#   1. CLI arguments
#   2. This config.yaml
#   3. Hard-coded defaults in each script's get_default_config()
#
# Sections:
#   [Shared]       execution, dataset, paths, sound, logging, telegram, plotting
#   [GA-specific]  genetic_algorithm, early_stop, cross_validation,
#                  multiprocessing, resource_monitor, model, caching,
#                  progress, export
#   [Stacking]     stacking, evaluation, models, automl, tsne
#   [WGAN-GP]      wgangp, training, generator, discriminator,
#                  gradient_penalty, generation, dataloader
# ================================================================================


# ==============================================================================
# SHARED SECTIONS — Used by multiple scripts
# ==============================================================================

# --- Execution Control (shared + per-script keys) ---
execution:
  verbose: false                      # Enable verbose output messages
  play_sound: true                    # Play sound notification when complete
  # genetic_algorithm.py specific:
  skip_train_if_model_exists: false   # Skip training if model artifacts exist
  runs: 5                             # Number of GA runs per population size
  resume_progress: true               # Resume from saved state files
  progress_save_interval: 10          # Save progress every N generations
  # stacking.py specific:
  csv_file: null                      # Optional single CSV file to process (null = process all)
  process_entire_dataset: false       # Process all files in dataset (true) or single file (false)
  test_data_augmentation: true        # Enable data augmentation testing experiments
  execution_mode: both                # Execution mode: "both" (default, runs binary and multi-class sequentially), "binary" (one file per eval), or "multi-class" (combined attacks)
  # wgangp.py specific:
  results_suffix: "_data_augmented"   # Suffix to add to generated filenames
  match_filenames_to_process: [""]    # List of specific filenames to match
  ignore_files: ["_data_augmented"]   # List of filename substrings to ignore
  ignore_dirs:                        # List of directory names to ignore
    - "Classifiers"
    - "Classifiers_Hyperparameters"
    - "Dataset_Description"
    - "Data_Separability"
    - "Feature_Analysis"

# --- Dataset Configuration (shared + per-script keys) ---
dataset:
  remove_zero_variance: true          # Remove zero-variance features during preprocessing
  # genetic_algorithm.py specific:
  files_to_ignore: []                 # List of files to ignore during processing
  test_size: 0.2                      # Test set size for train-test split
  min_test_fraction: 0.05             # Minimum acceptable test fraction
  max_test_fraction: 0.50             # Maximum acceptable test fraction
  # wgangp.py specific:
  label_candidates:                   # Common label column names for auto-detection
    - "label"
    - "class"
    - "target"
  datasets:                           # Dictionary containing dataset paths
    CICDDoS2019-Dataset:
      - "./Datasets/CICDDoS2019/01-12/"
      - "./Datasets/CICDDoS2019/03-11/"

# --- File Paths (shared + per-script keys) ---
paths:
  logs_dir: "./Logs"                  # Directory for log files (shared)
  # genetic_algorithm.py specific:
  datasets_dir: null                  # Directory containing datasets (null for auto-detect)
  output_dir: "Feature_Analysis"      # Output directory for GA results
  csv_path: null                      # Default dataset CSV path (null for auto-detect)
  # wgangp.py specific:
  out_dir: "outputs"                  # Output directory for WGAN-GP models/logs
  checkpoint_subdir: "Checkpoints"    # Checkpoints subdirectory name
  data_augmentation_subdir: "Data_Augmentation"  # Data augmentation subdirectory name

# --- Sound Notifications ---
sound:
  enabled: true                       # Enable sound notifications
  commands:                           # Commands to play sound for each operating system
    Darwin: "afplay"
    Linux: "aplay"
    Windows: "start"
  file: "./.assets/Sounds/NotificationSound.wav"  # Path to the sound file

# --- Logging ---
logging:
  enabled: true                       # Enable file logging
  clean: true                         # Clear log file on start
  tqdm_bar_format: "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"  # Progress bar format

# --- Telegram Notifications ---
telegram:
  enabled: true                       # Enable Telegram notifications
  verify_env: true                    # Verify .env file existence
  progress_pct: 10                    # Percent interval for progress notifications (e.g., 10 => notify at 10%,20%,...)

# --- Plotting / Visualization ---
plotting:
  enabled: true                       # Enable plot generation
  filename: "training_metrics.png"    # Plot filename
  subdir: "plots"                     # Subdirectory under data augmentation outputs for plots
  figsize: [18, 10]                   # Figure size [width, height]
  dpi: 300                            # Image resolution
  subplot_rows: 2                     # Number of subplot rows
  subplot_cols: 3                     # Number of subplot columns
  linewidth: 1.5                      # Line width for plots
  alpha: 0.7                          # Transparency for plot lines
  grid_alpha: 0.3                     # Grid transparency


# ==============================================================================
# GENETIC ALGORITHM SECTIONS — Used by genetic_algorithm.py
# ==============================================================================

genetic_algorithm:
  n_generations: 200                  # Number of generations for GA
  min_pop: 20                         # Minimum population size
  max_pop: 20                         # Maximum population size
  cxpb: 0.5                           # Crossover probability
  mutpb: 0.01                         # Mutation probability

early_stop:
  acc_threshold: 0.75                 # Minimum acceptable accuracy for an individual
  folds: 3                            # Number of folds to verify before early stopping
  generations: 10                     # Number of generations without improvement before early stop

cross_validation:
  n_folds: 10                         # Number of cross-validation folds

multiprocessing:
  n_jobs: -1                          # Number of parallel jobs (-1 uses all processors)
  cpu_processes: 1                    # Initial number of worker processes

resource_monitor:
  enabled: true                       # Enable resource monitoring
  interval_seconds: 30                # Interval between monitoring cycles in seconds
  reserve_cpu_frac: 0.15              # Fraction of CPU reserved from worker allocation
  reserve_mem_frac: 0.15              # Fraction of memory reserved from worker allocation
  min_procs: 1                        # Minimum number of processes allowed
  max_procs: null                     # Maximum number of processes allowed (null for unlimited)
  min_gens_before_update: 10          # Minimum GA generations before updating workers
  daemon: true                        # Whether the monitoring thread runs as daemon

model:
  estimator: "RandomForestClassifier" # Default estimator for GA fitness evaluation
  random_state: null                  # Random state for reproducibility (null for non-deterministic)

caching:
  enabled: true                       # Enable fitness caching
  pickle_protocol: 4                  # Pickle protocol to use when saving state

progress:
  state_dir_name: "ga_progress"       # Subfolder under Feature_Analysis to store progress files

export:
  results_csv_columns:                # Columns for the results CSV
    - timestamp
    - tool
    - run_index
    - model
    - dataset
    - hyperparameters
    - cv_method
    - train_test_split
    - scaling
    - cv_accuracy
    - cv_precision
    - cv_recall
    - cv_f1_score
    - test_accuracy
    - test_precision
    - test_recall
    - test_f1_score
    - test_fpr
    - test_fnr
    - feature_extraction_time_s
    - training_time_s
    - testing_time_s
    - elapsed_run_time
    - hardware
    - best_features
    - rfe_ranking


# ==============================================================================
# STACKING SECTIONS — Used by stacking.py
# ==============================================================================

stacking:
  # --- Output Filenames ---
  results_filename: "Stacking_Classifiers_Results.csv"  # Main results CSV filename for binary mode
  multiclass_results_filename: "Stacking_Classifiers_MultiClass_Results.csv"  # Results CSV filename for multi-class mode
  augmentation_comparison_filename: "Data_Augmentation_Comparison_Results.csv"  # Data augmentation experiments results CSV
  hyperparameters_filename: "Hyperparameter_Optimization_Results.csv"  # Hyperparameter tuning results filename

  # --- Data Augmentation Configuration ---
  data_augmentation_suffix: "_data_augmented"  # Suffix for augmented files (matches wgangp.py output)
  augmentation_ratios: [0.10, 0.25, 0.50, 0.75, 1.00]  # Augmented-to-original sample ratios for experiments

  # --- File Processing Configuration ---
  cache_prefix: "Cache_"              # Prefix for cached preprocessing artifacts
  model_export_base: "Feature_Analysis/Stacking/Models/"  # Base directory for model exports
  match_filenames_to_process: [""]    # List of specific filenames to match (empty = process all)
  ignore_files:                       # List of result filenames to ignore during processing
    - "Stacking_Classifiers_Results.csv"
    - "Hyperparameter_Optimization_Results.csv"
  ignore_dirs:                        # List of directory names to ignore during processing
    - "Classifiers"
    - "Classifiers_Hyperparameters"
    - "Dataset_Description"
    - "Data_Separability"
    - "Feature_Analysis"

  # --- Results CSV Schema ---
  # Column ordering for results CSV export. Each evaluated model writes one row with these fields.
  results_csv_columns:
    - "model"                  # Model identifier (e.g., "RandomForest", "SVM")
    - "dataset"                # Dataset name extracted from file path
    - "execution_mode"         # Execution mode: "binary" or "multi-class"
    - "attack_types_combined"  # For multi-class: comma-separated attack types; for binary: same as dataset
    - "feature_set"            # Feature extraction method (e.g., "PCA", "RFE", "GA")
    - "classifier_type"        # Classifier family (e.g., "tree", "ensemble", "neural")
    - "model_name"             # Full model class name
    - "data_source"            # Data source type: "original", "augmented", "combined"
    - "feature_selection_enabled"   # NEW: True/False for FS toggle
    - "hyperparameters_enabled"    # NEW: True/False for HP toggle
    - "data_augmentation_enabled"  # NEW: True/False for DA toggle
    - "experiment_id"          # Unique experiment identifier (timestamp-based)
    - "experiment_mode"        # Experiment type: "baseline", "augmentation", "comparison"
    - "augmentation_ratio"     # Ratio of augmented-to-original samples (null for baseline)
    - "n_features"             # Number of features in the dataset
    - "n_samples_train"        # Number of training samples
    - "n_samples_test"         # Number of testing samples
    - "accuracy"               # Classification accuracy (0.0-1.0)
    - "precision"              # Precision score (0.0-1.0)
    - "recall"                 # Recall score (0.0-1.0)
    - "f1_score"               # F1 score (0.0-1.0)
    - "fpr"                    # False Positive Rate (0.0-1.0)
    - "fnr"                    # False Negative Rate (0.0-1.0)
    - "elapsed_time_s"         # Elapsed time in seconds for evaluation
    - "cv_method"              # Cross-validation method (e.g., "StratifiedKFold")
    - "top_features"           # Top feature names (comma-separated or JSON)
    - "rfe_ranking"            # RFE feature rankings (comma-separated or JSON)
    - "hyperparameters"        # Model hyperparameters (JSON string)
    - "features_list"          # Complete feature list (comma-separated or JSON)
    - "Hardware"               # Hardware specification (CPU/GPU model)

  use_suffix_filenames: false         # When true, write separate CSVs per combo with suffix

# ==============================================================================
# EVALUATION CONFIGURATION — Used by stacking.py and other classifiers
# ==============================================================================

evaluation:
  n_jobs: -1                          # Number of parallel jobs for model training (-1 = use all available CPUs)
  threads_limit: 2                    # Thread limit for parallel classifier evaluation (prevents resource exhaustion)
  cv_folds: 10                        # Number of cross-validation folds for model evaluation
  random_state: 42                    # Random seed for reproducibility across all models
  ram_threshold_gb: 128               # RAM threshold (GB) to trigger threads_limit=1 (high-memory datasets)


# ==============================================================================
# MODEL HYPERPARAMETERS — Used by stacking.py
# ==============================================================================

models:
  # Random Forest classifier hyperparameters
  random_forest:
    n_estimators: 100                 # Number of trees in the forest
    random_state: 42                  # Random seed for reproducibility
  
  # Support Vector Machine hyperparameters
  svm:
    kernel: "rbf"                     # Kernel type: "linear", "poly", "rbf", "sigmoid"
    probability: true                 # Enable probability estimates for predict_proba()
    random_state: 42                  # Random seed for reproducibility
  
  # XGBoost classifier hyperparameters
  xgboost:
    eval_metric: "mlogloss"           # Evaluation metric: "mlogloss" for multi-class, "logloss" for binary
    random_state: 42                  # Random seed for reproducibility
  
  # Logistic Regression hyperparameters
  logistic_regression:
    max_iter: 1000                    # Maximum number of iterations for solver convergence
    random_state: 42                  # Random seed for reproducibility
  
  # K-Nearest Neighbors hyperparameters
  knn:
    n_neighbors: 5                    # Number of neighbors to consider
  
  # Gradient Boosting classifier hyperparameters
  gradient_boosting:
    random_state: 42                  # Random seed for reproducibility
  
  # LightGBM classifier hyperparameters
  lightgbm:
    force_row_wise: true              # Force row-wise histogram construction (faster for wide datasets)
    min_gain_to_split: 0.01           # Minimum information gain to make a split
    random_state: 42                  # Random seed for reproducibility
    verbosity: -1                     # Suppress warnings
  mlp:
    hidden_layer_sizes: [100]
    max_iter: 500
    random_state: 42
  stacking_meta:                      # Meta-estimator for stacking classifier
    n_estimators: 50
    random_state: 42

automl:
  enabled: false                      # Enable AutoML Optuna pipeline
  n_trials: 50                        # Number of Optuna trials for model search
  stacking_trials: 20                 # Number of Optuna trials for stacking search
  timeout: 3600                       # Timeout in seconds per search phase
  cv_folds: 5                         # Cross-validation folds for AutoML evaluation
  random_state: 42                    # Random seed for AutoML reproducibility
  stacking_top_n: 5                   # Number of top models to consider for stacking
  results_filename: "AutoML_Results.csv"  # AutoML results export filename

tsne:
  perplexity: 30                      # t-SNE perplexity parameter
  random_state: 42                    # Random seed for t-SNE
  n_iter: 1000                        # Number of t-SNE iterations
  figsize: [12, 10]                   # Figure size for t-SNE plots
  dpi: 300                            # DPI for t-SNE plots
  alpha: 0.6                          # Marker transparency
  marker_size: 50                     # Marker size for scatter plots

explainability:
  enabled: true                       # Enable model explainability (SHAP, LIME, permutation importance)
  shap: true                          # Enable SHAP explanations (TreeExplainer/LinearExplainer/KernelExplainer)
  lime: true                          # Enable LIME local interpretable model-agnostic explanations
  permutation_importance: true        # Enable permutation feature importance
  feature_importance: true            # Enable built-in model feature importance extraction
  pdp: false                          # Enable Partial Dependence Plots (advanced feature)
  ice: false                          # Enable Individual Conditional Expectation plots (advanced feature)
  surrogate: false                    # Enable surrogate model explanations (advanced feature)
  max_display_features: 20            # Maximum number of features to display in plots
  lime_num_samples: 1000              # Number of samples for LIME neighborhood generation
  shap_max_samples: 100               # Maximum test samples to use for SHAP (reduces computation time)
  random_state: 42                    # Random seed for explainability reproducibility


# ==============================================================================
# WGAN-GP SECTIONS — Used by wgangp.py
# ==============================================================================

wgangp:
  mode: "both"                        # Mode: train, gen, or both
  csv_path: null                      # Path to CSV training data (null for batch mode)
  label_col: "Label"                  # Column name for class label
  feature_cols: null                  # List of feature column names (null = use all)
  seed: 42                            # Random seed for reproducibility
  force_cpu: false                    # Force CPU usage even if CUDA available
  from_scratch: false                 # Force training from scratch, ignore checkpoints
  results_csv_columns:                # Columns for per-directory data augmentation results CSV
    - original_file
    - generated_file
    - original_num_samples
    - total_generated_samples
    - generated_ratio
    - epochs
    - batch_size
    - critic_iterations
    - lambda_gp
    - learning_rate_generator
    - learning_rate_critic
    - latent_dim
    - critic_loss
    - generator_loss
    - hardware
    - training_time_s
    - testing_time_s
  hardware_tracking: true

training:
  epochs: 60                          # Number of training epochs
  batch_size: 64                      # Training batch size
  critic_steps: 5                     # Number of discriminator updates per generator update
  lr: 0.0001                          # Learning rate for both networks
  beta1: 0.5                          # Adam optimizer beta1 parameter
  beta2: 0.9                          # Adam optimizer beta2 parameter
  lambda_gp: 10.0                     # Gradient penalty coefficient
  save_every: 5                       # Save checkpoints every N epochs
  log_interval: 50                    # Log metrics every N steps
  sample_batch: 16                    # Number of samples for fixed noise generation
  use_amp: false                      # Use automatic mixed precision
  compile: false                      # Use torch.compile() for faster execution

generator:
  latent_dim: 100                     # Dimensionality of noise vector
  hidden_dims: [256, 512]             # Hidden layer sizes
  embed_dim: 32                       # Label embedding dimension
  n_resblocks: 3                      # Number of residual blocks
  leaky_relu_alpha: 0.2               # LeakyReLU negative slope

discriminator:
  hidden_dims: [512, 256, 128]        # Hidden layer sizes
  embed_dim: 32                       # Label embedding dimension
  leaky_relu_alpha: 0.2               # LeakyReLU negative slope

gradient_penalty:
  epsilon: 1e-12                      # Small constant for numerical stability

generation:
  checkpoint: null                    # Path to generator checkpoint
  n_samples: 1.0                      # Number/percentage of samples to generate (1.0 = 100%)
  label: null                         # Specific class ID to generate (null = all classes)
  out_file: "generated.csv"           # Output CSV filename
  gen_batch_size: 256                 # Generation batch size
  feature_dim: null                   # Feature dimensionality (null = auto-detect)
  small_class_threshold: 100          # Threshold for small class detection
  small_class_min_samples: 10         # Minimum samples for small classes

dataloader:
  num_workers: 8                      # Number of workers for data loading
  pin_memory: true                    # Use pinned memory for faster GPU transfer
  persistent_workers: true            # Keep workers alive between epochs
  prefetch_factor: 2                  # Number of batches to prefetch
