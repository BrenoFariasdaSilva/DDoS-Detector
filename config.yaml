# ================================================================================
# DDoS-Detector — Unified Configuration File
# ================================================================================
# Central configuration for ALL project scripts:
#   - genetic_algorithm.py  (Genetic Algorithm Feature Selection)
#   - stacking.py           (Stacking Ensemble Classifiers with Multi-class Support)
#   - wgangp.py             (WGAN-GP Data Augmentation)
#
# Configuration priority (highest → lowest):
#   1. CLI arguments
#   2. This config.yaml
#   3. Hard-coded defaults in each script's get_default_config()
#
# Sections:
#   [Shared]       execution, dataset, paths, sound, logging, telegram, plotting
#   [GA-specific]  genetic_algorithm, early_stop, cross_validation,
#                  multiprocessing, resource_monitor, model, caching,
#                  progress, export
#   [Stacking]     stacking, evaluation, models, automl, tsne
#   [WGAN-GP]      wgangp, training, generator, discriminator,
#                  gradient_penalty, generation, dataloader
# ================================================================================


# ==============================================================================
# EXECUTION — Used by multiple scripts
# ==============================================================================
execution:  # Execution control and per-script overrides
  verbose: false                      # Enable verbose output messages
  play_sound: true                    # Play sound notification when complete
  skip_train_if_model_exists: false   # Skip training if model artifacts exist
  runs: 10                             # Number of GA runs per population size
  resume_progress: true               # Resume from saved state files
  progress_save_interval: 10          # Save progress every N generations
  csv_file: null                      # Optional single CSV file to process (null = process all)
  process_entire_dataset: false       # Process all files in dataset (true) or single file (false)
  test_data_augmentation: true        # Enable data augmentation testing experiments
  execution_mode: both                # Execution mode: "both" (default, runs binary and multi-class sequentially), "binary" (one file per eval), or "multi-class" (combined attacks)
  results_suffix: "_data_augmented"   # Suffix to add to generated filenames
  match_filenames_to_process: [""]    # List of specific filenames to match
  ignore_files: ["_data_augmented"]   # List of filename substrings to ignore
  ignore_dirs:                        # List of directory names to ignore
    - "Classifiers"
    - "Classifiers_Hyperparameters"
    - "Dataset_Description"
    - "Data_Separability"
    - "Feature_Analysis"

# ------------------------------------------------------------------------------
# Hyperparameters Optimization configuration (mandatory fields for script)
# ------------------------------------------------------------------------------
hyperparameters_optimization:
  execution:
    verbose: false
    n_jobs: -2
    skip_train_if_model_exists: false

  export:
    results_dir: "Feature_Analysis/Hyperparameter_Optimization"
    results_filename: "Hyperparameter_Optimization_Results.csv"
    model_export_base_dir: "Feature_Analysis/Hyperparameter_Optimization/Models"
    cache_prefix: "Cache_"

  dataset_processing:
    match_filenames_to_process: [""]
    ignore_files: ["Hyperparameter_Optimization_Results.csv"]
    ignore_dirs:
      - "Classifiers_Hyperparameters"
      - "Dataset_Description"
      - "Data_Separability"
      - "Feature_Analysis"

  models:
    enabled_models:
      - "Random Forest"
      - "SVM"
      - "XGBoost"
      - "Logistic Regression"
      - "KNN"
      - "Nearest Centroid"
      - "Gradient Boosting"
      - "LightGBM"
      - "MLP (Neural Net)"

  datasets:
    CICDDoS2019-Dataset:
      - "./Datasets/CICDDoS2019/01-12/"
      - "./Datasets/CICDDoS2019/03-11/"

  results_schema:
    columns:
      - "base_csv"
      - "model"
      - "best_params"
      - "best_cv_f1_score"
      - "n_features"
      - "feature_selection_method"
      - "dataset"
      - "elapsed_time_s"
      - "accuracy"
      - "precision"
      - "recall"
      - "fpr"
      - "fnr"
      - "tpr"
      - "tnr"
      - "matthews_corrcoef"
      - "roc_auc_score"
      - "cohen_kappa"
      - "Hardware"

# ==============================================================================
# DATASET — Used by multiple scripts
# ==============================================================================
dataset:  # Dataset-level settings and paths
  remove_zero_variance: true          # Remove zero-variance features during preprocessing
  files_to_ignore: []                 # List of files to ignore during processing
  test_size: 0.2                      # Test set size for train-test split
  min_test_fraction: 0.05             # Minimum acceptable test fraction
  max_test_fraction: 0.50             # Maximum acceptable test fraction
  label_candidates:                   # Common label column names for auto-detection
    - "label"
    - "class"
    - "target"
  datasets:                           # Dictionary containing dataset paths
    CICDDoS2019-Dataset:
      - "./Datasets/CICDDoS2019/01-12/"
      - "./Datasets/CICDDoS2019/03-11/"

# ==============================================================================
# PATHS — Used by multiple scripts
# ==============================================================================
paths:  # File and directory paths used across scripts
  logs_dir: "./Logs"                  # Directory for log files (shared)
  datasets_dir: null                  # Directory containing datasets (null for auto-detect)
  output_dir: "Feature_Analysis"      # Output directory for GA results
  csv_path: null                      # Default dataset CSV path (null for auto-detect)
  out_dir: "outputs"                  # Output directory for WGAN-GP models/logs
  checkpoint_subdir: "Checkpoints"    # Checkpoints subdirectory name
  data_augmentation_subdir: "Data_Augmentation"  # Data augmentation subdirectory name

# ==============================================================================
# SOUND — Used by multiple scripts
# ==============================================================================
sound:  # Sound notification settings
  enabled: true                       # Enable sound notifications
  commands:                           # Commands to play sound for each operating system
    Darwin: "afplay"
    Linux: "aplay"
    Windows: "start"
  file: "./.assets/Sounds/NotificationSound.wav"  # Path to the sound file

# ==============================================================================
# LOGGING — Used by multiple scripts
# ==============================================================================
logging:  # Logging configuration
  enabled: true                       # Enable file logging
  clean: true                         # Clear log file on start
  tqdm_bar_format: "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"  # Progress bar format

# ==============================================================================
# TELEGRAM — Used by multiple scripts
# ==============================================================================
telegram:  # Telegram notification settings
  enabled: true                       # Enable Telegram notifications
  verify_env: true                    # Verify .env file existence
  progress_pct: 10                    # Percent interval for progress notifications (e.g., 10 => notify at 10%,20%,...)

# ==============================================================================
# PLOTTING — Used by multiple scripts
# ==============================================================================
plotting:  # Plotting defaults and sizes
  enabled: true                       # Enable plot generation
  filename: "training_metrics.png"    # Plot filename
  subdir: "plots"                     # Subdirectory under data augmentation outputs for plots
  figsize: [18, 10]                   # Figure size [width, height]
  dpi: 300                            # Image resolution
  subplot_rows: 2                     # Number of subplot rows
  subplot_cols: 3                     # Number of subplot columns
  linewidth: 1.5                      # Line width for plots
  alpha: 0.7                          # Transparency for plot lines
  grid_alpha: 0.3                     # Grid transparency


# ==============================================================================
# GENETIC ALGORITHM — Used by genetic_algorithm.py
# ==============================================================================

genetic_algorithm:
  n_generations: 200                  # Number of generations for GA
  min_pop: 20                         # Minimum population size
  max_pop: 20                         # Maximum population size
  cxpb: 0.5                           # Crossover probability
  mutpb: 0.01                         # Mutation probability

early_stop:
  acc_threshold: 0.75                 # Minimum acceptable accuracy for an individual
  folds: 3                            # Number of folds to verify before early stopping
  generations: 10                     # Number of generations without improvement before early stop

cross_validation:
  n_folds: 10                         # Number of cross-validation folds

multiprocessing:
  n_jobs: -1                          # Number of parallel jobs (-1 uses all processors)
  cpu_processes: 1                    # Initial number of worker processes

resource_monitor:
  enabled: true                       # Enable resource monitoring
  interval_seconds: 30                # Interval between monitoring cycles in seconds
  reserve_cpu_frac: 0.15              # Fraction of CPU reserved from worker allocation
  reserve_mem_frac: 0.15              # Fraction of memory reserved from worker allocation
  min_procs: 1                        # Minimum number of processes allowed
  max_procs: null                     # Maximum number of processes allowed (null for unlimited)
  min_gens_before_update: 10          # Minimum GA generations before updating workers
  daemon: true                        # Whether the monitoring thread runs as daemon

model:
  estimator: "RandomForestClassifier" # Default estimator for GA fitness evaluation
  random_state: null                  # Random state for reproducibility (null for non-deterministic)

caching:
  enabled: true                       # Enable fitness caching
  pickle_protocol: 4                  # Pickle protocol to use when saving state

progress:
  state_dir_name: "ga_progress"       # Subfolder under Feature_Analysis to store progress files

export:
  results_dir: "Feature_Analysis/Genetic_Algorithm"
  results_filename: "Genetic_Algorithm_Results.csv"
  results_csv_columns:                # Columns for the results CSV
    - timestamp
    - tool
    - run_index
    - model
    - dataset
    - hyperparameters
    - cv_method
    - train_test_split
    - scaling
    - cv_accuracy
    - cv_precision
    - cv_recall
    - cv_f1_score
    - test_accuracy
    - test_precision
    - test_recall
    - test_f1_score
    - test_fpr
    - test_fnr
    - feature_extraction_time_s
    - training_time_s
    - testing_time_s
    - elapsed_run_time
    - hardware
    - best_features
    - union_features_across_runs
    - rfe_ranking


# ==============================================================================
# STACKING — Used by stacking.py
# ==============================================================================

stacking:  # Stacking experiment settings
  results_dir: "Stacking"             # Directory to save stacking results (relative to Feature_Analysis)
  results_filename: "Stacking_Classifiers_Results.csv"   # Main results CSV filename for binary mode
  multiclass_results_filename: "Stacking_Classifiers_MultiClass_Results.csv"  # Results CSV filename for multi-class mode
  augmentation_comparison_filename: "Data_Augmentation_Comparison_Results.csv"  # Data augmentation experiments results CSV
  hyperparameters_filename: "Hyperparameter_Optimization_Results.csv"  # Hyperparameter tuning results filename

  data_augmentation_suffix: "_data_augmented"  # Suffix for augmented files (matches wgangp.py output)
  augmentation_ratios: [0.10, 0.25, 0.50, 0.75, 1.00]  # Augmented-to-original sample ratios for experiments

  top_n_features_heatmap: 15            # Number of top features to show in generated heatmaps and CSV exports (int)

  cache_prefix: "Cache_"              # Prefix for cached preprocessing artifacts
  model_export_base: "Stacking/Models/"  # Base directory for model exports
  match_filenames_to_process: [""]    # List of specific filenames to match (empty = process all)
  ignore_files:                         # Result filenames to ignore during processing
    - "Stacking_Classifiers_Results.csv"
    - "Hyperparameter_Optimization_Results.csv"
  ignore_dirs:                          # Directory names to ignore during processing
    - "Classifiers"
    - "Classifiers_Hyperparameters"
    - "Dataset_Description"
    - "Data_Separability"
    - "Feature_Analysis"

  plots:
    plots_subdir: "Plots/"
    enabled: true
    dpi: 300
    format: "png"

  results_csv_columns:                 # Column ordering for results CSV export
    - "model"                  # Model identifier (e.g., "RandomForest", "SVM")
    - "dataset"                # Dataset name extracted from file path
    - "execution_mode"         # Execution mode: "binary" or "multi-class"
    - "attack_types_combined"  # For multi-class: comma-separated attack types; for binary: same as dataset
    - "feature_set"            # Feature extraction method (e.g., "PCA", "RFE", "GA")
    - "classifier_type"        # Classifier family (e.g., "tree", "ensemble", "neural")
    - "model_name"             # Full model class name
    - "data_source"            # Data source type: "original", "augmented", "combined"
    - "feature_selection_enabled"   # NEW: True/False for FS toggle
    - "hyperparameters_enabled"    # NEW: True/False for HP toggle
    - "data_augmentation_enabled"  # NEW: True/False for DA toggle
    - "experiment_id"          # Unique experiment identifier (timestamp-based)
    - "experiment_mode"        # Experiment type: "baseline", "augmentation", "comparison"
    - "augmentation_ratio"     # Ratio of augmented-to-original samples (null for baseline)
    - "n_features"             # Number of features in the dataset
    - "n_samples_train"        # Number of training samples
    - "n_samples_test"         # Number of testing samples
    - "accuracy"               # Classification accuracy (0.0-1.0)
    - "precision"              # Precision score (0.0-1.0)
    - "recall"                 # Recall score (0.0-1.0)
    - "f1_score"               # F1 score (0.0-1.0)
    - "fpr"                    # False Positive Rate (0.0-1.0)
    - "fnr"                    # False Negative Rate (0.0-1.0)
    - "elapsed_time_s"         # Elapsed time in seconds for evaluation
    - "cv_method"              # Cross-validation method (e.g., "StratifiedKFold")
    - "top_features"           # Top feature names (comma-separated or JSON)
    - "rfe_ranking"            # RFE feature rankings (comma-separated or JSON)
    - "hyperparameters"        # Model hyperparameters (JSON string)
    - "features_list"          # Complete feature list (comma-separated or JSON)
    - "Hardware"               # Hardware specification (CPU/GPU model)

  use_suffix_filenames: false         # When true, write separate CSVs per combo with suffix

# ==============================================================================
# EVALUATION — Used by stacking.py and other classifiers
# ==============================================================================

evaluation:  # Evaluation and CV defaults
  n_jobs: -1                          # Number of parallel jobs for model training (-1 = use all available CPUs)
  threads_limit: 2                    # Thread limit for parallel classifier evaluation (prevents resource exhaustion)
  cv_folds: 10                        # Number of cross-validation folds for model evaluation
  random_state: 42                    # Random seed for reproducibility across all models
  ram_threshold_gb: 128               # RAM threshold (GB) to trigger threads_limit=1 (high-memory datasets)


# ==============================================================================
# MODEL HYPERPARAMETERS — Used by stacking.py
# ==============================================================================

models:  # Model hyperparameters for classifiers
  random_forest:                       # Random Forest classifier hyperparameters
    n_estimators: 100                  # Number of trees in the forest
    random_state: 42                   # Random seed for reproducibility

  svm:                                 # Support Vector Machine hyperparameters
    kernel: "rbf"                     # Kernel type: "linear", "poly", "rbf", "sigmoid"
    probability: true                   # Enable probability estimates for predict_proba()
    random_state: 42                    # Random seed for reproducibility

  xgboost:                             # XGBoost classifier hyperparameters
    eval_metric: "mlogloss"           # Evaluation metric: "mlogloss" for multi-class, "logloss" for binary
    random_state: 42                    # Random seed for reproducibility

  logistic_regression:                 # Logistic Regression hyperparameters
    max_iter: 1000                      # Maximum iterations for solver convergence
    random_state: 42                    # Random seed for reproducibility

  knn:                                 # K-Nearest Neighbors hyperparameters
    n_neighbors: 5                      # Number of neighbors to consider

  gradient_boosting:                   # Gradient Boosting classifier hyperparameters
    random_state: 42                    # Random seed for reproducibility

  lightgbm:                            # LightGBM classifier hyperparameters
    force_row_wise: true                # Force row-wise histogram construction (bool)
    min_gain_to_split: 0.01             # Minimum information gain to make a split (float)
    random_state: 42                    # Random seed for reproducibility
    verbosity: -1                       # Suppress warnings (int)

  mlp:                                 # Multi-layer Perceptron hyperparameters
    hidden_layer_sizes: [100]           # Hidden layer sizes (list[int])
    max_iter: 500                       # Maximum training iterations (int)
    random_state: 42                    # Random seed for reproducibility

  stacking_meta:                       # Meta-estimator for stacking classifier
    n_estimators: 50                    # Number of estimators for stacking meta-estimator
    random_state: 42                    # Random seed for the stacking meta-estimator

automl:
  enabled: false                      # Enable AutoML Optuna pipeline
  n_trials: 50                        # Number of Optuna trials for model search
  stacking_trials: 20                 # Number of Optuna trials for stacking search
  timeout: 3600                       # Timeout in seconds per search phase
  cv_folds: 5                         # Cross-validation folds for AutoML evaluation
  random_state: 42                    # Random seed for AutoML reproducibility
  stacking_top_n: 5                   # Number of top models to consider for stacking
  results_filename: "AutoML_Results.csv"  # AutoML results export filename

tsne:
  perplexity: 30                      # t-SNE perplexity parameter
  random_state: 42                    # Random seed for t-SNE
  n_iter: 1000                        # Number of t-SNE iterations
  figsize: [12, 10]                   # Figure size for t-SNE plots
  dpi: 300                            # DPI for t-SNE plots
  alpha: 0.6                          # Marker transparency
  marker_size: 50                     # Marker size for scatter plots

explainability:
  enabled: true                       # Enable model explainability (SHAP, LIME, permutation importance)
  shap: true                          # Enable SHAP explanations (TreeExplainer/LinearExplainer/KernelExplainer)
  lime: true                          # Enable LIME local interpretable model-agnostic explanations
  permutation_importance: true        # Enable permutation feature importance
  feature_importance: true            # Enable built-in model feature importance extraction
  pdp: false                          # Enable Partial Dependence Plots (advanced feature)
  ice: false                          # Enable Individual Conditional Expectation plots (advanced feature)
  surrogate: false                    # Enable surrogate model explanations (advanced feature)
  max_display_features: 20            # Maximum number of features to display in plots
  lime_num_samples: 1000              # Number of samples for LIME neighborhood generation
  shap_max_samples: 100               # Maximum test samples to use for SHAP (reduces computation time)
  random_state: 42                    # Random seed for explainability reproducibility

# ==============================================================================
# PCA — Used by pca.py
# ==============================================================================
pca:
  execution:
    verbose: false
    skip_train_if_model_exists: false
    dataset_path: null

  model:
    estimator: "RandomForestClassifier"
    random_state: 42

  dimensionality:
    n_components: 8
    n_components_list: [8, 16, 32, 64]

  preprocessing:
    scale_data: true
    remove_zero_variance: true

  cross_validation:
    n_folds: 10

  multiprocessing:
    n_jobs: -1
    cpu_processes: 1

  caching:
    enabled: true
    pickle_protocol: 4

  export:
    results_csv_columns:
      - timestamp
      - tool
      - model
      - dataset
      - hyperparameters
      - cv_method
      - train_test_split
      - scaling
      - n_components
      - explained_variance
      - cv_accuracy
      - cv_precision
      - cv_recall
      - cv_f1_score
      - test_accuracy
      - test_precision
      - test_recall
      - test_f1_score
      - test_fpr
      - test_fnr
      - feature_extraction_time_s
      - training_time_s
      - testing_time_s
      - hardware

# ==============================================================================
# DATASET DESCRIPTOR — Used by dataset_descriptor.py
# ==============================================================================
dataset_descriptor:
  include_preprocessing_metrics: true        # Include preprocessing statistics
  include_data_augmentation_info: true      # Include WGANGP augmentation summary column
  generate_table_image: true                # Export table image using dataframe_image
  table_image_format: "png"                # Table image format (png/jpg)
  csv_output_suffix: "_description"        # Suffix for generated descriptor CSV
  class_column_name: "Label"               # Target/class column name
  dropna_before_analysis: false             # Drop NaN rows before computing metrics
  compute_class_distribution: true          # Compute per-class counts and proportions
  compute_feature_statistics: true          # Compute feature-level summary statistics
  round_decimals: 4                         # Number of decimal places for floats

# RFE — Used by rfe.py
# ==============================================================================
rfe:
  execution:
    verbose: false
    skip_train_if_model_exists: false
    dataset_path: null

  model:
    estimator: "random_forest"
    random_state: 42

  selection:
    n_features_to_select: 10
    step: 1

  cross_validation:
    n_folds: 10

  multiprocessing:
    n_jobs: -1
    cpu_processes: 1

  caching:
    enabled: true
    pickle_protocol: 4

  export:
    results_dir: "Feature_Analysis/RFE"
    results_filename: "RFE_Results.csv"
    results_csv_columns:
      - timestamp
      - tool
      - model
      - dataset
      - hyperparameters
      - cv_method
      - train_test_split
      - scaling
      - cv_accuracy
      - cv_precision
      - cv_recall
      - cv_f1_score
      - test_accuracy
      - test_precision
      - test_recall
      - test_f1_score
      - test_fpr
      - test_fnr
      - feature_extraction_time_s
      - training_time_s
      - testing_time_s
      - hardware
      - top_features
      - rfe_ranking


# ==============================================================================
# WGAN-GP — Used by wgangp.py
# ==============================================================================

wgangp:
  mode: "both"                        # Mode: train, gen, or both
  csv_path: null                      # Path to CSV training data (null for batch mode)
  label_col: "Label"                  # Column name for class label
  feature_cols: null                  # List of feature column names (null = use all)
  seed: 42                            # Random seed for reproducibility
  force_cpu: false                    # Force CPU usage even if CUDA available
  from_scratch: false                 # Force training from scratch, ignore checkpoints
  results_dir: "Feature_Analysis/PCA"
  results_filename: "PCA_Results.csv"
  results_csv_columns:                # Columns for per-directory data augmentation results CSV
    - original_file
    - generated_file
    - original_num_samples
    - total_generated_samples
    - generated_ratio
    - epochs
    - batch_size
    - critic_iterations
    - lambda_gp
    - latent_dim
    - learning_rate_generator
    - learning_rate_critic
    - epoch
    - epoch_time_s
    - critic_loss
    - generator_loss
    - gp
    - D_real
    - D_fake
    - wasserstein
    - training_time_s
    - file_time_s
    - testing_time_s
    - hardware
  hardware_tracking: true

training:
  epochs: 60                          # Number of training epochs
  batch_size: 64                      # Training batch size
  critic_steps: 5                     # Number of discriminator updates per generator update
  lr: 0.0001                          # Learning rate for both networks
  beta1: 0.5                          # Adam optimizer beta1 parameter
  beta2: 0.9                          # Adam optimizer beta2 parameter
  lambda_gp: 10.0                     # Gradient penalty coefficient
  save_every: 5                       # Save checkpoints every N epochs
  log_interval: 50                    # Log metrics every N steps
  sample_batch: 16                    # Number of samples for fixed noise generation
  use_amp: false                      # Use automatic mixed precision
  compile: false                      # Use torch.compile() for faster execution

generator:
  latent_dim: 100                     # Dimensionality of noise vector
  hidden_dims: [256, 512]             # Hidden layer sizes
  embed_dim: 32                       # Label embedding dimension
  n_resblocks: 3                      # Number of residual blocks
  leaky_relu_alpha: 0.2               # LeakyReLU negative slope

discriminator:
  hidden_dims: [512, 256, 128]        # Hidden layer sizes
  embed_dim: 32                       # Label embedding dimension
  leaky_relu_alpha: 0.2               # LeakyReLU negative slope

gradient_penalty:
  epsilon: 1e-12                      # Small constant for numerical stability

generation:
  checkpoint: null                    # Path to generator checkpoint
  n_samples: 1.0                      # Number/percentage of samples to generate (1.0 = 100%)
  label: null                         # Specific class ID to generate (null = all classes)
  force_new_samples: false            # When true, always regenerate samples even if output exists
  out_file: "generated.csv"           # Output CSV filename
  gen_batch_size: 256                 # Generation batch size
  feature_dim: null                   # Feature dimensionality (null = auto-detect)
  small_class_threshold: 100          # Threshold for small class detection
  small_class_min_samples: 10         # Minimum samples for small classes

dataloader:
  num_workers: 8                      # Number of workers for data loading
  pin_memory: true                    # Use pinned memory for faster GPU transfer
  persistent_workers: true            # Keep workers alive between epochs
  prefetch_factor: 2                  # Number of batches to prefetch
